{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python-Web_Scraping.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPVUCp11D0no+RBYVLaNFfJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PariSsy/parissy.github.io/blob/master/Python_Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0if_mQW0zhT"
      },
      "source": [
        "# Web Scraping in Python\n",
        "\n",
        "* Instructor = Thomas Laetsch, DS at NYU\n",
        "* [Course Link](https://learn.datacamp.com/courses/web-scraping-with-python)\n",
        "* Notes taken = Aug 4, 2021\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA1ObSLA1qyf"
      },
      "source": [
        "# Chapter 1 - Intro to HTML\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSES9DJT-Zh5"
      },
      "source": [
        "## 1.1 - HyperText Markup\n",
        "\n",
        "```\n",
        "<html>\n",
        "    <body>\n",
        "        <div>\n",
        "            <p>Hello World!</p>\n",
        "            <p>Enjoy DataCamp!</p>\n",
        "        </div>\n",
        "        <p>Thanks for Watching!</p>\n",
        "    </body>\n",
        "</html>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gPnXkH9-ed_"
      },
      "source": [
        "## 1.2 - HTML Tags and Attributes\n",
        "\n",
        "1. Tag names: **html**, **div**, and **p**\n",
        "  * `<tag-name attrib-name=\"attrib info\"> ..element contents.. </tag-name>`\n",
        "  * `<div id=\"unique-id\" class=\"some class\"> ..div element contents.. </div>`\n",
        "  * **id** attribute should be unique\n",
        "  * **class** attribute doesn't need to be unique\n",
        "2. **a** tags are for hyperlinks; **href** attribute tells what link to go to\n",
        "  * `<a href=\"https://www.datacamp.com\"> This text links to DataCamp! </a>`\n",
        "\n",
        "See tag traction on w3schools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jAadksS-gfP"
      },
      "source": [
        "## 1.3 - Crash Course X\n",
        "\n",
        "1. Direct to all `table` elements within the entire HTML code: `xpath = '//table'`\n",
        "2. Direct to all `table` elements which are descendants of the 2nd `div` child of the `body` element: `xpath = '/html/body/div[2]//table'`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU6_izpM8Ocd"
      },
      "source": [
        "# Chapter 2 - XPaths and Selectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFO3O650-jPv"
      },
      "source": [
        "## 2.1 - XPathology\n",
        "\n",
        "### Slashes and Brackets\n",
        "\n",
        "* `/` looks forward ONE generation\n",
        "* `//` looks forward all future generations\n",
        "* `[]` narrows in specific elements\n",
        "* `*` asterisk is the wildcard\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THbG_Muh-lsO"
      },
      "source": [
        "## 2.2 - Off the Beaten XPath\n",
        "\n",
        "### (At)tribute\n",
        "* `@` represents \"attribute\": `@class`, `@id`, `@href`\n",
        "* Examples:\n",
        "  + `xpath = '//p[@class=\"class-1\"]'`\n",
        "  + `xpath = '//*[@id=\"uid\"]'`\n",
        "  + `xpath = '//div[@id=\"uid\"]/p[2]'`\n",
        "\n",
        "### Example\n",
        "```\n",
        "<html>\n",
        "  <body>\n",
        "    <div id=\"uid\">\n",
        "      <p class=\"class-1\">Hello World!</p>\n",
        "      <p class=\"class-2\">Enjoy DataCamp!</p>\n",
        "    </div>\n",
        "    </p class=\"class-1\">Thanks for Watching!</p>\n",
        "  </body>\n",
        "</html>\n",
        "```\n",
        "\n",
        "### Content with Contains\n",
        "Xpath contains notation: `contains(@attri-name, \"string-expr\")`\n",
        "\n",
        "1. `xpath = '//*[contains(@class,\"class-1\")]'`\n",
        "2. `xpath = '//*[@class=\"class-1\"]'`\n",
        "3. `xpath = '/html/body/div/p[2]/@class'`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXY4Dso5-nhw"
      },
      "source": [
        "## 2.3 - scrapy Selector Objects\n",
        "\n",
        "### Setting up a Selector\n",
        "```\n",
        "from scrapy import Selector\n",
        "\n",
        "html = '''\n",
        "<html>\n",
        "  <body>\n",
        "    <div class=\"hello datacamp\">\n",
        "      <p>Hello World!</p>\n",
        "    </div>\n",
        "    <p>Enjoy DataCamp!</p>\n",
        "  </body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "sel = Selector( text = html )\n",
        "```\n",
        "\n",
        "### Selecting Selectors\n",
        "\n",
        "Use **`xpath`** within **`Selector`**:\n",
        "```\n",
        "sel.xpath(\"//p\")\n",
        "# outputs the SelectorList:\n",
        "[<Selector xpath='//p' data='<p>Hello World!</p>'>,\n",
        " <Selector xpath='//p' data='<p>Enjoy DataCamp!</p>'>]\n",
        "```\n",
        "\n",
        "### Extracting Data from a SelectorList\n",
        "\n",
        "**`extract()`**\n",
        "```\n",
        ">>> sel.xpath(\"//p\").extract()\n",
        "out: [ '<p>Hello World!</p>',\n",
        "       '<p>Enjoy DataCamp!</p>' ]\n",
        "```\n",
        "\n",
        "**`extract_first()`**\n",
        "```\n",
        ">>> sel.xpath(\"//p\").extract_first()\n",
        "out: '<p>Hello World!</p>'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie8ZvkSd-pWm"
      },
      "source": [
        "## 2.4 - Inspecting the Source\n",
        "\n",
        "### HTML text to Selector\n",
        "\n",
        "```\n",
        "from scrapy import Selector\n",
        "import requests\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/Web_scraping'\n",
        "html = requests.get( url ).content\n",
        "sel = Selector( text = html )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5cLu6E_37S2"
      },
      "source": [
        "# Chapter 3 - CSS Locators, Chaining, and Responses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYrewVnL-JLY"
      },
      "source": [
        "## 3.1 - CSS Locators\n",
        "\n",
        "1. General rules\n",
        "  + `/` replaced by `>` (except the first character)\n",
        "    + XPath: `/html/body/div`\n",
        "    + CSS Locator: `html > body > div`\n",
        "  + `//` replaced by a blank space (except the first character)\n",
        "    + XPath: `//div/span//p`\n",
        "    + CSS Locator: `div > span p`\n",
        "  + `[N]` replaced by `:nth-of-type(N)`\n",
        "    + XPath: `//div/p[2]`\n",
        "    + CSS Locator: `div > p:nth-of-type(2)`\n",
        "2. Conversion example\n",
        "  + XPATH: `xpath = '/html/body//div/p[2]'`\n",
        "  + CSS: `css = 'html > body div > p:nth-of-type(2)'`\n",
        "3. Attributes in CSS\n",
        "  + `.` to find an element by *class*: `p.class-1` selects all paragraph elements belonging to `class-1`\n",
        "  + `#` to find an element by *id*: `div#uid` selects the `div` element with `id` = `uid`\n",
        "  + Examples:\n",
        "    + Select paragraph elements within class `class1`: `css_locator = 'div#uid > p.class1'`\n",
        "    + Select all elements whose class attribute belongs to `class1`: `css_locator = '.class1'`\n",
        "\n",
        "### Selectors with CSS\n",
        "\n",
        "```\n",
        ">>> sel.css(\"div > p\")\n",
        "out: [<Selector xpath='...' data='<p>Hello World!</p>'>]\n",
        "\n",
        ">>> sel.css(\"div > p\").extract()\n",
        "out: [ '<p>Hello World!</p>' ]\n",
        "```\n",
        "\n",
        "### More conversion examples:\n",
        "\n",
        "```\n",
        "xpath = '/html/body/span[1]//a'\n",
        "css_locator = 'html > body > span:nth-of-type(1) a'\n",
        "\n",
        "xpath = '//div[@id=\"uid\"]/span//h4'\n",
        "css_locator = 'div#uid > span h4'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTcs38MP-Kk3"
      },
      "source": [
        "## 3.2 - Attribute and Text Selection\n",
        "\n",
        "1. XPath: `<xpath-to-element>/@attr-name` - e.g., `xpath = '//div[@id=\"uid]/a/@href'`\n",
        "2. CSS Locator: `<css-to-element>::attr(attr-name)` - e.g., `css_locator = 'div#uid > a::attr(href)'`\n",
        "\n",
        "### Text Extraction\n",
        "\n",
        "```\n",
        "<p id=\"p-example\">\n",
        "  Hello world!\n",
        "  Try <a href=\"http://www.datacamp.com\">DataCamp</a> today!\n",
        "</p>\n",
        "```\n",
        "\n",
        "* In XPath use `text()`\n",
        "\n",
        "```\n",
        "sel.xpath('//p[@id=\"p-example\"]/text()').extract()\n",
        "# result: ['\\n Hello world!\\n Try ',' today!\\n']\n",
        "\n",
        "sel.xpath('//p[@id=\"p-example\"]//text()').extract()\n",
        "# result: ['\\n Hello world!\\n Try ','DataCamp',' today!\\n']\n",
        "```\n",
        "\n",
        "* In CSS Lacator use `::text`\n",
        "\n",
        "```\n",
        "sel.css('p#p-example::text').extract()\n",
        "# result: ['\\n Hello world!\\n Try ',' today!\\n']\n",
        "\n",
        "sel.css('p#p-example ::text').extract()\n",
        "# result: ['\\n Hello world!\\n Try ','DataCamp',' today!\\n']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts4PCxBs-L74"
      },
      "source": [
        "## 3.3 - `response`\n",
        "\n",
        "Review:\n",
        "1. `xpath` method works like a Selector - `response.xpath('//div/span[@class=\"bio\"]')`\n",
        "2. `css` method works like a Selector - `response.css( 'div > span.bio' )`\n",
        "3. Chaining works like a Selector - `response.xpath('//div').css('span.bio')`\n",
        "4. Data extraction works like a Selector - `response.xpath('//div').css('span.bio').extract_first()`\n",
        "\n",
        "New:\n",
        "1. The `response` keeps track of the URL within the response url variable\n",
        "```\n",
        "response.url\n",
        ">>> 'http://www.DataCamp.com/courses/all'\n",
        "```\n",
        "2. The `response` lets us `follow()` a new link\n",
        "```\n",
        "# next_url is the string path of the next url we want to scrape\n",
        "response.follow( next_url )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BDP1kP_Md3z"
      },
      "source": [
        "## 3.3 DataCamp Course Link Scraping\n",
        "\n",
        "DataCamp Site: https://www.datacamp.com/courses/all  \n",
        "(By the creation of this course, there were 185 DataCamp courses; by the creation of this notebook, there were 356 DataCamp courses)\n",
        "\n",
        "Response loaded with HTML from https://www.datacamp.com/courses/all\n",
        "```\n",
        "course_divs = response.css('div.course-block')\n",
        "print( len(course_divs) )\n",
        ">>> 185\n",
        "```\n",
        "\n",
        "### Inspecting course-block\n",
        "```\n",
        "first_div = course_divs[0]\n",
        "children = first_div.xpath('./*')\n",
        "print( len(children) )\n",
        ">>> 3\n",
        "```\n",
        "### The first child\n",
        "```\n",
        "first_div = course_divs[0]\n",
        "children = first_div.xpath('./*')\n",
        "first_child = children[0]\n",
        "print( first_child.extract() )\n",
        ">>> <a class=... />\n",
        "```\n",
        "\n",
        "### The second child\n",
        "```\n",
        "first_div = course_divs[0]\n",
        "children = first_div.xpath('./*')\n",
        "second_child = children[1]\n",
        "print( second_child.extract() )\n",
        ">>> <div class=... />\n",
        "```\n",
        "\n",
        "### The forgotten child\n",
        "```\n",
        "first_div = course_divs[0]\n",
        "children = first_div.xpath('./*')\n",
        "third_child = children[2]\n",
        "print( third_child.extract() )\n",
        ">>> <span class=... />\n",
        "```\n",
        "\n",
        "### Listful\n",
        "* In one CSS Locator\n",
        "```\n",
        "links = response.css('div.course-block > a::attr(href)').extract()\n",
        "```\n",
        "* Stepwise\n",
        "```\n",
        "# step 1: course blocks\n",
        "course_divs = response.css('div.course-block')\n",
        "# step 2: hyperlink elements\n",
        "hrefs = course_divs.xpath('./a/@href')\n",
        "# step 3: extract the links\n",
        "links = hrefs.extract()\n",
        "```\n",
        "\n",
        "### Get Schooled\n",
        "\n",
        "```\n",
        "for l in links:\n",
        "print( l )\n",
        "\n",
        ">>> /courses/free-introduction-to-r\n",
        ">>> /courses/data-table-data-manipulation-r-tutorial\n",
        ">>> /courses/dplyr-data-manipulation-r-tutorial\n",
        ">>> /courses/ggvis-data-visualization-r-tutorial\n",
        ">>> /courses/reporting-with-r-markdown\n",
        ">>> /courses/intermediate-r\n",
        "\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rygClKCM37cG"
      },
      "source": [
        "# Chapter 4 - Spiders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tU0napW-RAX"
      },
      "source": [
        "## 4.1 - A Classy Spider\n",
        "\n",
        "```\n",
        "import scrapy\n",
        "from scrpy.crawler import CrawlerProcess\n",
        "\n",
        "class SpiderClassName(scrapy.Spider):\n",
        "    name = \"Spider_name\"\n",
        "    # the code for your spider\n",
        "\n",
        "# Initiate a CrawlerProcess\n",
        "process = CrawlerProcess()\n",
        "\n",
        "# tell the process which spider to use\n",
        "process.crawl(SpiderClassName)\n",
        "\n",
        "# start the crawling process\n",
        "process.start()\n",
        "```\n",
        "\n",
        "### Weaving the Web\n",
        "\n",
        "```\n",
        "class DCspider( scrapy.Spider ):\n",
        "\n",
        "    name = 'dc_spider'\n",
        "\n",
        "    def start_requests( self ):\n",
        "        urls = ['https://www.datacamp.com/courses/all']\n",
        "        for url in urls:\n",
        "            yield scrapy.Request( url = url, callback = self.parse )\n",
        "    \n",
        "    def parse( self, response ):\n",
        "        # simple example: write out the html\n",
        "        html_file = 'DC_courses.html'\n",
        "        with open( html_file, 'wb' ) as fout:\n",
        "            fout.write( response.body )\n",
        "```\n",
        "\n",
        "* Need to have a function called `start_requests`\n",
        "* Need to have at least 1 parser function to handle the HTML code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR3ZljfN-Sev"
      },
      "source": [
        "## Section 4.3\n",
        "\n",
        "### DataCamp Course Links: Save to File\n",
        "\n",
        "```\n",
        "class DCspider( scrapy.Spider ):\n",
        "    name = \"dcspider\"\n",
        "\n",
        "    def start_requests( self ):\n",
        "        urls = [ 'https://www.datacamp.com/courses/all' ]\n",
        "        for url in urls:\n",
        "            yield scrapy.Request( url = url, callback = self.parse)\n",
        "    def parse( self, response ):\n",
        "        links = response.css('div.course-block > a::attr(href)').extract()\n",
        "        filepath = 'DC_links.csv'\n",
        "        with open( filepath, 'w' ) as f:\n",
        "            f.writelines( [link + '/n' for link in links] )\n",
        "```\n",
        "\n",
        "### DataCamp Course Links: Parse Again\n",
        "\n",
        "```\n",
        "class DCspider( scrapy.Spider ):\n",
        "    name = \"dcspider\"\n",
        "\n",
        "    def start_requests( self ):\n",
        "        urls = [ 'https://www.datacamp.com/courses/all' ]\n",
        "        for url in urls:\n",
        "            yield scrapy.Request( url = url, callback = self.parse)\n",
        "    def parse( self, response ):\n",
        "        links = response.css('div.course-block > a::attr(href)').extract()\n",
        "        for link in links:\n",
        "            yield response.follow( url = link, callback = self.parse2)\n",
        "    def parse2( self, response ):\n",
        "        # parse the course sites here\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVSINhQx-T7Q"
      },
      "source": [
        "## Section 4.4\n",
        "\n",
        "### Inspecting Elements\n",
        "\n",
        "```\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "class DC_Chapter_Spider(scrapy.Spider):\n",
        "\n",
        "    name = \"dc_chapter_spider\"\n",
        "\n",
        "    def start_requests( self ):\n",
        "        url = 'https://www.datacamp.com/courses/all'\n",
        "        yield scrapy.Request( url = url, callback = self.parse_front)\n",
        "\n",
        "    def parse_front( self, response ):\n",
        "        ## Code to parse the front courses page\n",
        "\n",
        "    def parse_pages( self, response ):\n",
        "        ## Code to parse course pages\n",
        "        ## Fill in dc_dict here\n",
        "\n",
        "dc_dict = dict()\n",
        "\n",
        "process = CrawlerProcess()\n",
        "process.crawl(DC_Chapter_Spider)\n",
        "process.start()\n",
        "```\n",
        "\n",
        "### Parsing the Front Page\n",
        "\n",
        "```\n",
        "def parse_front( self, response ):\n",
        "\n",
        "    # Narrow in on the course blocks\n",
        "    course_blocks = response.css( 'div.course-block' )\n",
        "\n",
        "    # Direct to the course links\n",
        "    course_links = course_blocks.xpath( './a/@href' )\n",
        "    \n",
        "    # Extract the links (as a list of strings)\n",
        "    links_to_follow = course_links.extract()\n",
        "\n",
        "    # Follow the links to the next parser\n",
        "    for url in links_to_follow:\n",
        "        yield response.follow(url = url, callback = self.parse_pages)\n",
        "```\n",
        "\n",
        "### Parsing the Course Pages\n",
        "\n",
        "```\n",
        "def parse_pages( self, response ):\n",
        "\n",
        "    # Direct to the course title text\n",
        "    crs_title = response.xpath( '//h1[contains(@class,\"title\")]/text()' )\n",
        "\n",
        "    # Extract and clean the course title text\n",
        "    crs_title_ext = crs_title.extract_first().strip()\n",
        "\n",
        "    # Direct to the chapter titles text\n",
        "    ch_titles = response.css( 'h4.chapter__title::text' )\n",
        "\n",
        "    # Extract and clean the chapter titles text\n",
        "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
        "    \n",
        "    # Store this in our dictionary\n",
        "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZsc7rCuVQcV"
      },
      "source": [
        "# !pip install \"scrapy\"\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        " \n",
        "class SpiderClassName(scrapy.Spider):\n",
        "    name = \"Spider_name\"\n",
        "    # the code for your spider\n",
        " \n",
        "# Initiate a CrawlerProcess\n",
        "process = CrawlerProcess()\n",
        " \n",
        "# tell the process which spider to use\n",
        "process.crawl(SpiderClassName)\n",
        " \n",
        "# start the crawling process\n",
        "process.start()\n",
        "\n",
        "class DCspider( scrapy.Spider ):\n",
        " \n",
        "    name = 'dc_spider'\n",
        " \n",
        "    def start_requests( self ):\n",
        "        urls = ['https://www.datacamp.com/courses/all']\n",
        "        for url in urls:\n",
        "            yield scrapy.Request( url = url, callback = self.parse )\n",
        " \n",
        "    def parse( self, response ):\n",
        "        # simple example: write out the html\n",
        "        html_file = 'DC_courses.html'\n",
        "        with open( html_file, 'wb' ) as fout:\n",
        "            fout.write( response.body )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqlbSU0azDP5"
      },
      "source": [
        "# # Import scrapy\n",
        "# import scrapy\n",
        "# # Import the CrawlerProcess: for running the spider\n",
        "# from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Create the Spider class\n",
        "class DC_Chapter_Spider(scrapy.Spider):\n",
        "  name = \"dc_chapter_spider\"\n",
        "  # start_requests method\n",
        "  def start_requests(self):\n",
        "    yield scrapy.Request(url = url_short,\n",
        "                         callback = self.parse_front)\n",
        "  # First parsing method\n",
        "  def parse_front(self, response):\n",
        "    course_blocks = response.css('div.course-block')\n",
        "    course_links = course_blocks.xpath('./a/@href')\n",
        "    links_to_follow = course_links.extract()\n",
        "    for url in links_to_follow:\n",
        "      yield response.follow(url = url,\n",
        "                            callback = self.parse_pages)\n",
        "  # Second parsing method\n",
        "  def parse_pages(self, response):\n",
        "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
        "    crs_title_ext = crs_title.extract_first().strip()\n",
        "    ch_titles = response.css('h4.chapter__title::text')\n",
        "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
        "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
        "\n",
        "# Initialize the dictionary **outside** of the Spider class\n",
        "dc_dict = dict()\n",
        "\n",
        "# Run the Spider\n",
        "process = CrawlerProcess()\n",
        "process.crawl(DC_Chapter_Spider)\n",
        "process.start()\n",
        "\n",
        "# Print a preview of courses\n",
        "previewCourses(dc_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdz26Xwez52X"
      },
      "source": [
        "## Parse DataCamp Course Descriptions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m16JMmGz-e-"
      },
      "source": [
        "# # Import scrapy\n",
        "# import scrapy\n",
        "# # Import the CrawlerProcess: for running the spider\n",
        "# from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Create the Spider class\n",
        "class DC_Description_Spider(scrapy.Spider):\n",
        "  name = \"dc_chapter_spider\"\n",
        "  # start_requests method\n",
        "  def start_requests(self):\n",
        "    yield scrapy.Request(url = url_short,\n",
        "                         callback = self.parse_front)\n",
        "  # First parsing method\n",
        "  def parse_front(self, response):\n",
        "    course_blocks = response.css('div.course-block')\n",
        "    course_links = course_blocks.xpath('./a/@href')\n",
        "    links_to_follow = course_links.extract()\n",
        "    for url in links_to_follow:\n",
        "      yield response.follow(url = url,\n",
        "                            callback = self.parse_pages)\n",
        "  # Second parsing method\n",
        "  def parse_pages(self, response):\n",
        "    # Create a SelectorList of the course titles text\n",
        "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
        "    # Extract the text and strip it clean\n",
        "    crs_title_ext = crs_title.extract_first().strip()\n",
        "    # Create a SelectorList of course descriptions text\n",
        "    crs_descr = response.css( 'p.course__description::text' )\n",
        "    # Extract the text and strip it clean\n",
        "    crs_descr_ext = crs_descr.extract_first().strip()\n",
        "    # Fill in the dictionary\n",
        "    dc_dict[crs_title_ext] = crs_descr_ext\n",
        "\n",
        "# Initialize the dictionary **outside** of the Spider class\n",
        "dc_dict = dict()\n",
        "\n",
        "# Run the Spider\n",
        "process = CrawlerProcess()\n",
        "process.crawl(DC_Description_Spider)\n",
        "process.start()\n",
        "\n",
        "# Print a preview of courses\n",
        "previewCourses(dc_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfrsZLfA135i"
      },
      "source": [
        "## Capstone Crawler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Lzcs-v16e2"
      },
      "source": [
        "# # Import scrapy\n",
        "# import scrapy\n",
        "# # Import the CrawlerProcess\n",
        "# from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Create the Spider class\n",
        "class YourSpider( scrapy.Spider ):\n",
        "  name = 'yourspider'\n",
        "  # start_requests method\n",
        "  def start_requests( self ):\n",
        "    yield scrapy.Request(url = url_short, callback = self.parse)\n",
        "      \n",
        "  def parse(self, response):\n",
        "    # My version of the parser you wrote in the previous part\n",
        "    crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
        "    crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
        "    for crs_title, crs_descr in zip( crs_titles, crs_descrs ):\n",
        "      dc_dict[crs_title] = crs_descr\n",
        "    \n",
        "# Initialize the dictionary **outside** of the Spider class\n",
        "dc_dict = dict()\n",
        "\n",
        "# Run the Spider\n",
        "process = CrawlerProcess()\n",
        "process.crawl(YourSpider)\n",
        "process.start()\n",
        "\n",
        "# Print a preview of courses\n",
        "previewCourses(dc_dict)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}